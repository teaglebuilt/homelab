---
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - ollama-pvc.yaml
  - ollama-httproute.yaml
helmCharts:
  - name: ollama
    releaseName: ollama
    repo: https://otwld.github.io/ollama-helm/
    version: 1.14.0
    namespace: ai
    valuesInline:
      ollama:
        gpu:
          enabled: true
          type: 'nvidia'
          number: 1
          nvidiaResource: "nvidia.com/gpu"
        models:
          pull:
            - firefunction-v2
            - nomic-embed-text
            - qwen3
            - gemma3
        insecure: true
      extraEnv:
        - name: OLLAMA_KV_CACHE_TYPE
          value: "q8_0"
        - name: OLLAMA_NUM_PARALLEL
          value: "1"
        - name: OLLAMA_FLASH_ATTENTION
          value: "1"
        - name: OLLAMA_DEBUG
          value: "1"
      hostNetwork: true
      runtimeClassName: nvidia
      nodeSelector:
        nvidia.com/gpu.present: "true"
      persistentVolume:
        enabled: true
        existingClaim: ollama-pvc
        storageClass: nfs-csi
        accessModes:
          - ReadWriteMany
        size: 200Gi
      requests:
        cpu: 60m
        memory: 256Mi
        nvidia.com/gpu: 1
      limits:
        cpu: 600m
        memory: 2Gi
        nvidia.com/gpu: 1
      podSecurityContext:
        fsGroupChangePolicy: Always
        fsGroup: 988
      containerSecurityContext:
        runAsUser: 977
        runAsGroup: 988
        runAsNonRoot: true
        privileged: true
        allowPrivilegeEscalation: true
        readOnlyRootFilesystem: false
        capabilities: { drop: ["ALL"] }
        seccompProfile: { type: RuntimeDefault }

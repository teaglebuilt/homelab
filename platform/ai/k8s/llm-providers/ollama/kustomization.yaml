---
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - ollama-pvc.yaml
  - backend.yaml
  - ollama-httproute.yaml
  - models
helmCharts:
  - name: ollama
    releaseName: ollama
    repo: https://otwld.github.io/ollama-helm/
    version: 1.31.0
    namespace: ai
    valuesInline:
      ollama:
        gpu:
          enabled: true
          type: 'nvidia'
          number: 1
        models:
          pull:
            - mistral
            - nomic-embed-text
        insecure: true
      hostNetwork: true
      runtimeClassName: nvidia
      service:
        annotations:
          service.cilium.io/global: "true"
          service.cilium.io/shared: "true"
      nodeSelector:
        nvidia.com/gpu.present: "true"
      persistentVolume:
        enabled: true
        existingClaim: ollama-pvc
        storageClass: nfs-csi
        accessModes:
          - ReadWriteMany
        size: 200Gi
      resources:
        requests:
          cpu: "1"
          memory: "2Gi"
          nvidia.com/gpu: 1
        limits:
          cpu: "2"
          memory: "12Gi"
          nvidia.com/gpu: 1
      podAnnotations:
        kubernetes.io/egress-bandwidth: "500M"
        kubernetes.io/ingress-bandwidth: "500M"
      podSecurityContext:
        fsGroupChangePolicy: Always
        fsGroup: 0
      containerSecurityContext:
        runAsUser: 977
        runAsGroup: 0
        runAsNonRoot: true
        privileged: true
        allowPrivilegeEscalation: true
        readOnlyRootFilesystem: false
        capabilities:
          drop: ["ALL"]
        seccompProfile:
          type: RuntimeDefault

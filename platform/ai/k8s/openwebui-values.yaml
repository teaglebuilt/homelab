---
ollama:
  enabled: true
  gpu:
    enabled: true
    type: 'nvidia'
    number: 1
  models:
    - llama3
    - codellama
  runtimeClassName: nvidia
  securityContext:
    allowPrivilegeEscalation: false
    capabilities:
      drop:
        - ALL
    runAsNonRoot: true
    seccompProfile:
      type: RuntimeDefault
  resources:
    requests:
      cpu: 200m
      memory: 1Gi
      nvidia.com/gpu: 1
    limits:
      memory: 8Gi
      nvidia.com/gpu: 1
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
          - matchExpressions:
              - key: feature.node.kubernetes.io/pci-0300_10de.present
                operator: In
                values:
                  - "true"
persistence:
  enabled: true
  # existingClaim: open-webui
pipelines:
  enabled: false
ollamaUrls: ["http://ollama.ai.svc.cluster.local:11434"]
# ingress:
#   enabled: true
#   class: "nginx"
#   annotations:
#     cert-manager.io/cluster-issuer: cloudflare-issuer
#   host: ollama.local
#   tls: true
  